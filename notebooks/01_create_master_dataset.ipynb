{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# çœ‹è­·å¸«é›¢è·ç‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆETLå®Œå…¨ç‰ˆï¼‰\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€æ‰‹ä½œæ¥­ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ä½œæˆã¨åŒç­‰ã®å“è³ªï¼ˆæ­£ç¢ºãªæ•°å€¤ã€é©åˆ‡ãªå˜ä½ï¼‰ã‚’è‡ªå‹•åŒ–ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚\n",
    "ç‰¹ã«**ã€Œæ²–ç¸„çœŒã®ãƒ‡ãƒ¼ã‚¿ç•°å¸¸ã€ã€Œå¹´åã®å˜ä½å¤‰æ›ã€ã€Œå®¶è³ƒã®æ°‘å–¶å€Ÿå®¶æŒ‡å®šã€**ãªã©ã®é‡è¦ãƒ­ã‚¸ãƒƒã‚¯ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "## å‰æ\n",
    "- å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆCSV/Excelï¼‰ãŒ `../data/raw/` ã«é…ç½®ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚\n",
    "- å‡ºåŠ›å…ˆã¯ `../data/processed/` ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Project Root: /Users/hideomi.h/nurse-turnover-analysis\n",
      "ğŸ“‚ Raw Data Dir: /Users/hideomi.h/nurse-turnover-analysis/data/raw\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "# ==========================================\n",
    "# 1. ãƒ‘ã‚¹è¨­å®š\n",
    "# ==========================================\n",
    "# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å ´æ‰€ã‚’åŸºæº–ã«ã€ä¸€ã¤ä¸Šã®éšå±¤ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¨ã™ã‚‹\n",
    "# ä¾‹: notebooks/01_create_master.ipynb -> project_root/\n",
    "BASE_DIR = Path(\"..\").resolve()\n",
    "\n",
    "# ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹\n",
    "RAW_DIR = BASE_DIR / \"data\" / \"raw\"\n",
    "PROCESSED_DIR = BASE_DIR / \"data\" / \"processed\"\n",
    "\n",
    "# å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "OUTPUT_FILE = PROCESSED_DIR / \"master_nurse_turnover_v2.csv\"\n",
    "\n",
    "print(f\"ğŸ“‚ Project Root: {BASE_DIR}\")\n",
    "print(f\"ğŸ“‚ Raw Data Dir: {RAW_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found: æ—¥æœ¬çœ‹è­·å”ä¼š_é›¢è·ç‡_éƒ½é“åºœçœŒåˆ¥_2023.csv\n",
      "âœ… Found: æ—¥æœ¬çœ‹è­·å”ä¼š_å¤œå‹¤72hè¶…éç‡_éƒ½é“åºœçœŒåˆ¥_2024.csv\n",
      "âœ… Found: åšåŠ´çœ_è¡›ç”Ÿè¡Œæ”¿å ±å‘Šä¾‹_çœ‹è­·å¸«æ•°_2023.csv\n",
      "âœ… Found: åšåŠ´çœ_åŒ»ç™‚æ–½è¨­èª¿æŸ»_ç—…åºŠè¦æ¨¡_2024.csv\n",
      "âœ… Found: ç·å‹™çœ_ä½å®…åœŸåœ°çµ±è¨ˆèª¿æŸ»_å®¶è³ƒ_2024.xlsx\n",
      "âœ… Found: ç·å‹™çœ_ç¤¾ä¼šç”Ÿæ´»çµ±è¨ˆæŒ‡æ¨™_äººå£å¯†åº¦_2023.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª\n",
    "# ==========================================\n",
    "# å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§\n",
    "FILES = {\n",
    "    \"turnover\": RAW_DIR / \"æ—¥æœ¬çœ‹è­·å”ä¼š_é›¢è·ç‡_éƒ½é“åºœçœŒåˆ¥_2023.csv\",\n",
    "    \"night_shift\": RAW_DIR / \"æ—¥æœ¬çœ‹è­·å”ä¼š_å¤œå‹¤72hè¶…éç‡_éƒ½é“åºœçœŒåˆ¥_2024.csv\",\n",
    "    \"nurse_count\": RAW_DIR / \"åšåŠ´çœ_è¡›ç”Ÿè¡Œæ”¿å ±å‘Šä¾‹_çœ‹è­·å¸«æ•°_2023.csv\",\n",
    "    \"hospital\": RAW_DIR / \"åšåŠ´çœ_åŒ»ç™‚æ–½è¨­èª¿æŸ»_ç—…åºŠè¦æ¨¡_2024.csv\",\n",
    "    \"rent\": RAW_DIR / \"ç·å‹™çœ_ä½å®…åœŸåœ°çµ±è¨ˆèª¿æŸ»_å®¶è³ƒ_2024.xlsx\",\n",
    "    \"density\": RAW_DIR / \"ç·å‹™çœ_ç¤¾ä¼šç”Ÿæ´»çµ±è¨ˆæŒ‡æ¨™_äººå£å¯†åº¦_2023.csv\"\n",
    "}\n",
    "\n",
    "missing = []\n",
    "for key, path in FILES.items():\n",
    "    if not path.exists():\n",
    "        missing.append(path.name)\n",
    "        print(f\"âŒ Missing: {path.name}\")\n",
    "    else:\n",
    "        print(f\"âœ… Found: {path.name}\")\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing}\\nãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading Turnover & Income Data...\n",
      "âš ï¸ Warning: 'annual_income' column not found. Check CSV headers.\n",
      "   Columns: ['prefecture', 'turnover_total', 'turnover_new_grad', 'turnover_experienced']\n",
      "   -> Loaded 48 rows.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. é›¢è·ç‡ãƒ»å¹´åãƒ‡ãƒ¼ã‚¿ (Base Master)\n",
    "# ==========================================\n",
    "print(\"ğŸ”„ Loading Turnover & Income Data...\")\n",
    "\n",
    "df_turnover = pd.read_csv(FILES[\"turnover\"])\n",
    "\n",
    "# â˜…ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ: åˆ—åã®å¼·åˆ¶ãƒªãƒãƒ¼ãƒ ï¼ˆæ—¥æœ¬èª -> è‹±èªï¼‰\n",
    "# å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ—åãŒå¤šå°‘é•ã£ã¦ã‚‚å‹•ãã‚ˆã†ã«ã€æƒ³å®šã•ã‚Œã‚‹åå‰ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™\n",
    "rename_map = {\n",
    "    \"éƒ½é“åºœçœŒ\": \"prefecture\",\n",
    "    \"é›¢è·ç‡\": \"turnover_total\",\n",
    "    \"æ­£è¦é›‡ç”¨è·å“¡é›¢è·ç‡\": \"turnover_total\",\n",
    "    \"æ–°å’é›¢è·ç‡\": \"turnover_new_grad\",\n",
    "    \"æ—¢å’é›¢è·ç‡\": \"turnover_experienced\",\n",
    "    \"ç¨è¾¼å¹´å\": \"annual_income\",\n",
    "    \"å¹´å\": \"annual_income\",\n",
    "    \"çµ¦ä¸\": \"annual_income\"\n",
    "}\n",
    "df_turnover = df_turnover.rename(columns=rename_map)\n",
    "\n",
    "# æ­£è¦åŒ–\n",
    "df_turnover[\"prefecture\"] = df_turnover[\"prefecture\"].apply(normalize_pref)\n",
    "\n",
    "# ã€é‡è¦ã€‘å¹´åã®å˜ä½å¤‰æ›ï¼ˆåƒå†† -> ä¸‡å††ï¼‰\n",
    "if \"annual_income\" in df_turnover.columns:\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãŒæ–‡å­—åˆ—ã«ãªã£ã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ã‹ã‚‰å‰²ã‚Šç®—\n",
    "    df_turnover[\"annual_income\"] = pd.to_numeric(df_turnover[\"annual_income\"], errors=\"coerce\")\n",
    "    # æ˜ã‚‰ã‹ã«åƒå††å˜ä½ï¼ˆ1000ä»¥ä¸Šï¼‰ã®å ´åˆã®ã¿å‰²ã‚‹\n",
    "    if df_turnover[\"annual_income\"].mean() > 1000:\n",
    "        df_turnover[\"annual_income\"] = df_turnover[\"annual_income\"] / 10\n",
    "        print(\"   -> Converted Annual Income unit (Thousands -> Man-yen)\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: 'annual_income' column not found. Check CSV headers.\")\n",
    "    print(f\"   Columns: {list(df_turnover.columns)}\")\n",
    "\n",
    "# ãƒã‚¹ã‚¿ã¨ãªã‚‹éƒ½é“åºœçœŒãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "MASTER_PREFS = set(df_turnover[\"prefecture\"].unique())\n",
    "\n",
    "print(f\"   -> Loaded {len(df_turnover)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading Turnover & Income Data...\n",
      "   -> Loaded 48 rows.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. é›¢è·ç‡ãƒ»å¹´åãƒ‡ãƒ¼ã‚¿ (Base Master)\n",
    "# ==========================================\n",
    "print(\"ğŸ”„ Loading Turnover & Income Data...\")\n",
    "\n",
    "df_turnover = pd.read_csv(FILES[\"turnover\"])\n",
    "df_turnover[\"prefecture\"] = df_turnover[\"prefecture\"].apply(normalize_pref)\n",
    "\n",
    "# ã€é‡è¦ã€‘å¹´åã®å˜ä½å¤‰æ›ï¼ˆåƒå†† -> ä¸‡å††ï¼‰\n",
    "# å…ƒãƒ‡ãƒ¼ã‚¿ãŒã€Œ4789(åƒå††)ã€ã®å ´åˆ -> ã€Œ478.9(ä¸‡å††)ã€ã«ã™ã‚‹\n",
    "if \"annual_income\" in df_turnover.columns:\n",
    "    df_turnover[\"annual_income\"] = df_turnover[\"annual_income\"] / 10\n",
    "\n",
    "# ãƒã‚¹ã‚¿ã¨ãªã‚‹éƒ½é“åºœçœŒãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "MASTER_PREFS = set(df_turnover[\"prefecture\"].unique())\n",
    "\n",
    "print(f\"   -> Loaded {len(df_turnover)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading Night Shift Data...\n",
      "   -> Loaded 48 rows.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. å¤œå‹¤è² æ‹…ãƒ‡ãƒ¼ã‚¿\n",
    "# ==========================================\n",
    "print(\"ğŸ”„ Loading Night Shift Data...\")\n",
    "\n",
    "df_night = pd.read_csv(FILES[\"night_shift\"])\n",
    "df_night[\"prefecture\"] = df_night[\"prefecture\"].apply(normalize_pref)\n",
    "\n",
    "print(f\"   -> Loaded {len(df_night)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading Nurse Count Data...\n",
      "   -> Loaded 47 rows.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. çœ‹è­·å¸«æ•°ãƒ‡ãƒ¼ã‚¿ (åšåŠ´çœ)\n",
    "# ==========================================\n",
    "print(\"ğŸ”„ Loading Nurse Count Data...\")\n",
    "\n",
    "# header=3 (4è¡Œç›®) ã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
    "df_nurse = pd.read_csv(FILES[\"nurse_count\"], header=3, encoding=\"cp932\")\n",
    "\n",
    "# 1åˆ—ç›®(éƒ½é“åºœçœŒ)ã¨4åˆ—ç›®(ç·æ•°)ã‚’æŠ½å‡º\n",
    "df_nurse = df_nurse.iloc[:, [0, 3]]\n",
    "df_nurse.columns = [\"prefecture\", \"nurse_count\"]\n",
    "\n",
    "# æ­£è¦åŒ–ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "df_nurse[\"prefecture\"] = df_nurse[\"prefecture\"].apply(normalize_pref)\n",
    "df_nurse[\"nurse_count\"] = df_nurse[\"nurse_count\"].apply(clean_number)\n",
    "\n",
    "# ãƒã‚¹ã‚¿ã«ã‚ã‚‹éƒ½é“åºœçœŒã®ã¿æ®‹ã™\n",
    "df_nurse = df_nurse[df_nurse[\"prefecture\"].isin(MASTER_PREFS)]\n",
    "\n",
    "print(f\"   -> Loaded {len(df_nurse)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading Hospital Data (Robust Logic)...\n",
      "   -> Validation:\n",
      "      åŒ—æµ·é“: Total=534, Large=17\n",
      "      æ²–ç¸„çœŒ: Total=3753, Large=240\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 7. ç—…é™¢æ•°ãƒ»ç—…åºŠè¦æ¨¡ (â˜…æœ€é‡è¦ãƒ­ã‚¸ãƒƒã‚¯)\n",
    "# ==========================================\n",
    "print(\"ğŸ”„ Loading Hospital Data (Robust Logic)...\")\n",
    "\n",
    "# åˆ—ã‚ºãƒ¬é˜²æ­¢ã®ãŸã‚ã€å¼·åˆ¶çš„ã«100åˆ—ç¢ºä¿ã—ã¦èª­ã¿è¾¼ã‚€\n",
    "df_hosp_raw = pd.read_csv(FILES[\"hospital\"], encoding=\"utf-8-sig\", header=None, dtype=str, names=range(100))\n",
    "\n",
    "# å¿…è¦ãªåˆ—ã‚’æŠ½å‡º\n",
    "# Col 8 (Iåˆ—): éƒ½é“åºœçœŒå\n",
    "# Col 9 (Jåˆ—): ã‚³ãƒ¼ãƒ‰ (1=ç·æ•°, 11-15=å¤§è¦æ¨¡)\n",
    "# Col 13 (Nåˆ—): æ–½è¨­æ•°\n",
    "df_wk = pd.DataFrame({\n",
    "    \"pref_raw\": df_hosp_raw[8],\n",
    "    \"code\": df_hosp_raw[9],\n",
    "    \"val_raw\": df_hosp_raw[13]\n",
    "})\n",
    "\n",
    "# 1. éƒ½é“åºœçœŒåã®ç‰¹å®šï¼ˆãƒã‚¹ã‚¿ã¨ã®ç…§åˆï¼‰\n",
    "def resolve_pref(text):\n",
    "    text = normalize_pref(text)\n",
    "    if not text: return None\n",
    "    # \"å…¨å›½\" \"å†æ²\" ãªã©ã‚’é™¤å¤–\n",
    "    if text in [\"å…¨å›½\", \"ç·æ•°\", \"å†æ²\"]: return None\n",
    "    # å‰æ–¹ä¸€è‡´ã§éƒ½é“åºœçœŒã‚’ç‰¹å®š\n",
    "    for p in MASTER_PREFS:\n",
    "        if p.startswith(text): return p\n",
    "    return None\n",
    "\n",
    "df_wk[\"pref_key\"] = df_wk[\"pref_raw\"].apply(resolve_pref)\n",
    "\n",
    "# 2. ffillã§å†…è¨³è¡Œã«éƒ½é“åºœçœŒåã‚’ã‚³ãƒ”ãƒ¼\n",
    "df_wk[\"prefecture\"] = df_wk[\"pref_key\"].ffill()\n",
    "\n",
    "# 3. æœ‰åŠ¹ãªè¡Œï¼ˆéƒ½é“åºœçœŒãŒã‚ã‚‹è¡Œï¼‰ã®ã¿æŠ½å‡º\n",
    "df_wk = df_wk.dropna(subset=[\"prefecture\"])\n",
    "\n",
    "# 4. æ•°å€¤åŒ–\n",
    "df_wk[\"count\"] = df_wk[\"val_raw\"].apply(clean_number)\n",
    "df_wk[\"code\"] = pd.to_numeric(df_wk[\"code\"], errors=\"coerce\")\n",
    "\n",
    "# 5. é›†è¨ˆ\n",
    "# ã€ç—…é™¢ç·æ•°ã€‘ Code == 1\n",
    "df_total = df_wk[df_wk[\"code\"] == 1].groupby(\"prefecture\")[\"count\"].sum().reset_index()\n",
    "df_total.columns = [\"prefecture\", \"hospital_count\"]\n",
    "\n",
    "# ã€å¤§è¦æ¨¡ç—…é™¢æ•°ã€‘ Code in [11, 12, 13, 14, 15] (500åºŠä»¥ä¸Š)\n",
    "large_codes = [11, 12, 13, 14, 15]\n",
    "df_large = df_wk[df_wk[\"code\"].isin(large_codes)].groupby(\"prefecture\")[\"count\"].sum().reset_index()\n",
    "df_large.columns = [\"prefecture\", \"large_hospital_count\"]\n",
    "\n",
    "# çµåˆ\n",
    "df_hospital = pd.merge(df_total, df_large, on=\"prefecture\", how=\"left\").fillna(0)\n",
    "\n",
    "# æ¤œè¨¼ï¼ˆåŒ—æµ·é“ã¨æ²–ç¸„ï¼‰\n",
    "print(\"   -> Validation:\")\n",
    "for p in [\"åŒ—æµ·é“\", \"æ²–ç¸„çœŒ\"]:\n",
    "    if p in df_hospital[\"prefecture\"].values:\n",
    "        row = df_hospital[df_hospital[\"prefecture\"]==p].iloc[0]\n",
    "        print(f\"      {p}: Total={row['hospital_count']}, Large={row['large_hospital_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading Hospital Data (Fixed Logic)...\n",
      "   -> Validation:\n",
      "      åŒ—æµ·é“: Total=534, Large=17\n",
      "      æ²–ç¸„çœŒ: Total=89, Large=2\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 7. ç—…é™¢æ•°ãƒ»ç—…åºŠè¦æ¨¡ (â˜…ä¿®æ­£ç‰ˆãƒ­ã‚¸ãƒƒã‚¯)\n",
    "# ==========================================\n",
    "print(\"ğŸ”„ Loading Hospital Data (Fixed Logic)...\")\n",
    "\n",
    "# å¼·åˆ¶çš„ã«100åˆ—ç¢ºä¿\n",
    "df_hosp_raw = pd.read_csv(FILES[\"hospital\"], encoding=\"utf-8-sig\", header=None, dtype=str, names=range(100))\n",
    "\n",
    "df_wk = pd.DataFrame({\n",
    "    \"pref_raw\": df_hosp_raw[8],  # Iåˆ—\n",
    "    \"code\": df_hosp_raw[9],      # Jåˆ—\n",
    "    \"val_raw\": df_hosp_raw[13]   # Nåˆ—\n",
    "})\n",
    "\n",
    "# 1. éƒ½é“åºœçœŒåã®ç‰¹å®šï¼ˆã‚¹ãƒˆãƒƒãƒ‘ãƒ¼æ©Ÿèƒ½ä»˜ãï¼‰\n",
    "def resolve_pref(text):\n",
    "    text = normalize_pref(text)\n",
    "    if not text: return None\n",
    "    # â˜…ã“ã“ãŒä¿®æ­£ç‚¹: ã€Œå…¨å›½ã€ã‚„ã€Œè¨ˆã€ãŒæ¥ãŸã‚‰ 'STOP' ã¨ã„ã†ãƒãƒ¼ã‚«ãƒ¼ã‚’è¿”ã™\n",
    "    if any(x in text for x in [\"å…¨å›½\", \"ç·æ•°\", \"å†æ²\", \"è¨ˆ\"]):\n",
    "        return \"STOP\"\n",
    "    # éƒ½é“åºœçœŒã¨ã®ãƒãƒƒãƒãƒ³ã‚°\n",
    "    for p in MASTER_PREFS:\n",
    "        if p.startswith(text): return p\n",
    "    return None\n",
    "\n",
    "df_wk[\"pref_key\"] = df_wk[\"pref_raw\"].apply(resolve_pref)\n",
    "\n",
    "# 2. ffillï¼ˆSTOPãƒãƒ¼ã‚«ãƒ¼ã‚‚å«ã‚ã¦åŸ‹ã‚ã‚‹ï¼‰\n",
    "df_wk[\"prefecture\"] = df_wk[\"pref_key\"].ffill()\n",
    "\n",
    "# 3. STOPãƒãƒ¼ã‚«ãƒ¼ã‚„ã‚´ãƒŸè¡Œã‚’é™¤å¤–\n",
    "# \"STOP\" ã«ãªã£ãŸè¡Œï¼ˆï¼å…¨å›½è¨ˆãªã©ï¼‰ã¨ãã®é…ä¸‹ã®è¡Œã‚’ã“ã“ã§æ¨ã¦ã‚‹\n",
    "df_wk = df_wk[df_wk[\"prefecture\"].isin(MASTER_PREFS)]\n",
    "\n",
    "# 4. æ•°å€¤åŒ– & é›†è¨ˆ\n",
    "df_wk[\"count\"] = df_wk[\"val_raw\"].apply(clean_number)\n",
    "df_wk[\"code\"] = pd.to_numeric(df_wk[\"code\"], errors=\"coerce\")\n",
    "\n",
    "# ç·æ•° (Code=1)\n",
    "df_total = df_wk[df_wk[\"code\"] == 1].groupby(\"prefecture\")[\"count\"].sum().reset_index()\n",
    "df_total.columns = [\"prefecture\", \"hospital_count\"]\n",
    "\n",
    "# å¤§è¦æ¨¡ (Code=11~15)\n",
    "large_codes = [11, 12, 13, 14, 15]\n",
    "df_large = df_wk[df_wk[\"code\"].isin(large_codes)].groupby(\"prefecture\")[\"count\"].sum().reset_index()\n",
    "df_large.columns = [\"prefecture\", \"large_hospital_count\"]\n",
    "\n",
    "# çµåˆ\n",
    "df_hospital = pd.merge(df_total, df_large, on=\"prefecture\", how=\"left\").fillna(0)\n",
    "\n",
    "# æ¤œè¨¼ï¼ˆæ²–ç¸„ãŒ 89 ã«ãªã‚Œã°æˆåŠŸï¼‰\n",
    "print(\"   -> Validation:\")\n",
    "for p in [\"åŒ—æµ·é“\", \"æ²–ç¸„çœŒ\"]:\n",
    "    if p in df_hospital[\"prefecture\"].values:\n",
    "        row = df_hospital[df_hospital[\"prefecture\"]==p].iloc[0]\n",
    "        print(f\"      {p}: Total={row['hospital_count']}, Large={row['large_hospital_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading Density Data...\n",
      "   -> Loaded 48 rows.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 9. äººå£ãƒ»äººå£å¯†åº¦\n",
    "# ==========================================\n",
    "print(\"ğŸ”„ Loading Density Data...\")\n",
    "\n",
    "# 13è¡Œç›®ã‹ã‚‰èª­ã¿è¾¼ã¿\n",
    "df_dens_raw = pd.read_csv(FILES[\"density\"], encoding=\"utf-8-sig\", header=12)\n",
    "\n",
    "# ã‚«ãƒ©ãƒ åã§æŠ½å‡ºï¼ˆå¤‰å‹•ã«å¯¾å¿œï¼‰\n",
    "col_pref = [c for c in df_dens_raw.columns if \"åœ°åŸŸ\" in c][0]\n",
    "col_pop = [c for c in df_dens_raw.columns if \"ç·äººå£\" in c][0] # ä¸‡äººå˜ä½\n",
    "col_dens = [c for c in df_dens_raw.columns if \"å¯†åº¦\" in c][0]\n",
    "\n",
    "df_density = df_dens_raw[[col_pref, col_pop, col_dens]].copy()\n",
    "df_density.columns = [\"prefecture\", \"population\", \"population_density\"]\n",
    "\n",
    "df_density[\"prefecture\"] = df_density[\"prefecture\"].apply(normalize_pref)\n",
    "for c in [\"population\", \"population_density\"]:\n",
    "    df_density[c] = df_density[c].apply(clean_number)\n",
    "\n",
    "print(f\"   -> Loaded {len(df_density)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Merging all datasets...\n",
      "âœ… Merge Complete.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 10. ãƒ‡ãƒ¼ã‚¿çµåˆã¨æŒ‡æ¨™è¨ˆç®—\n",
    "# ==========================================\n",
    "print(\"ğŸ”„ Merging all datasets...\")\n",
    "\n",
    "# é›¢è·ç‡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹\n",
    "df_master = df_turnover.copy()\n",
    "\n",
    "# é †æ¬¡ãƒãƒ¼ã‚¸\n",
    "merge_list = [df_night, df_nurse, df_hospital, df_rent, df_density]\n",
    "for df in merge_list:\n",
    "    df_master = pd.merge(df_master, df, on=\"prefecture\", how=\"left\")\n",
    "\n",
    "# --- è¨ˆç®—æŒ‡æ¨™ ---\n",
    "\n",
    "# 1. å¤§è¦æ¨¡ç—…é™¢æ¯”ç‡ (%)\n",
    "# 0é™¤ç®—å›é¿\n",
    "df_master[\"large_hospital_ratio\"] = df_master.apply(\n",
    "    lambda x: (x[\"large_hospital_count\"] / x[\"hospital_count\"] * 100) if x[\"hospital_count\"] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 2. äººå£ã‚ãŸã‚Šä¿‚æ•° (Population Unit)\n",
    "# äººå£ã¯ã€Œä¸‡äººã€å˜ä½ãªã®ã§ã€10ã§å‰²ã‚‹ã¨ã€Œ10ä¸‡äººå˜ä½ã€ã«ãªã‚‹\n",
    "# ä¾‹: 100ä¸‡äºº / 10 = 10 (10ä¸‡ãŒ10å€‹)\n",
    "df_master[\"pop_unit_100k\"] = df_master[\"population\"] / 10\n",
    "\n",
    "# 3. äººå£10ä¸‡äººã‚ãŸã‚Šçœ‹è­·å¸«æ•°\n",
    "df_master[\"nurse_per_100k\"] = df_master[\"nurse_count\"] / df_master[\"pop_unit_100k\"]\n",
    "\n",
    "# 4. äººå£10ä¸‡äººã‚ãŸã‚Šç—…é™¢æ•°\n",
    "df_master[\"hospital_per_100k\"] = df_master[\"hospital_count\"] / df_master[\"pop_unit_100k\"]\n",
    "\n",
    "# ä¸è¦ãªä¸€æ™‚ã‚«ãƒ©ãƒ å‰Šé™¤\n",
    "df_master.drop(columns=[\"pop_unit_100k\"], inplace=True)\n",
    "\n",
    "print(\"âœ… Merge Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Warning: Missing values detected:\n",
      "nurse_count              1\n",
      "hospital_count           1\n",
      "large_hospital_count     1\n",
      "rent_private             1\n",
      "population              48\n",
      "population_density      48\n",
      "nurse_per_100k          48\n",
      "hospital_per_100k       48\n",
      "dtype: int64\n",
      "\n",
      "ğŸ” Okinawa Verification:\n",
      "  Hospital Count: 89.0 (Expected: 89)\n",
      "  Large Hospital: 2.0 (Expected: 2)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'annual_income'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'annual_income'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Hospital Count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhospital_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Expected: 89)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Large Hospital: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlarge_hospital_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Expected: 2)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Income: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannual_income\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Expected: ~471)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ä¿å­˜\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ã‚«ãƒ©ãƒ ä¸¦ã³æ›¿ãˆ\u001b[39;00m\n\u001b[1;32m     23\u001b[0m cols_order \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefecture\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mturnover_total\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mturnover_new_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mturnover_experienced\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopulation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpopulation_density\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'annual_income'"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 11. æœ€çµ‚ç¢ºèªã¨ä¿å­˜\n",
    "# ==========================================\n",
    "\n",
    "# æ¬ æå€¤ãƒã‚§ãƒƒã‚¯\n",
    "null_counts = df_master.isnull().sum()\n",
    "if null_counts.sum() > 0:\n",
    "    print(\"âš ï¸ Warning: Missing values detected:\")\n",
    "    print(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(\"âœ… No missing values.\")\n",
    "\n",
    "# æ²–ç¸„ãƒã‚§ãƒƒã‚¯ (é‡è¦)\n",
    "if \"æ²–ç¸„çœŒ\" in df_master[\"prefecture\"].values:\n",
    "    row = df_master[df_master[\"prefecture\"]==\"æ²–ç¸„çœŒ\"].iloc[0]\n",
    "    print(\"\\nğŸ” Okinawa Verification:\")\n",
    "    print(f\"  Hospital Count: {row['hospital_count']} (Expected: 89)\")\n",
    "    print(f\"  Large Hospital: {row['large_hospital_count']} (Expected: 2)\")\n",
    "    print(f\"  Income: {row['annual_income']} (Expected: ~471)\")\n",
    "\n",
    "# ä¿å­˜\n",
    "# ã‚«ãƒ©ãƒ ä¸¦ã³æ›¿ãˆ\n",
    "cols_order = [\n",
    "    \"prefecture\",\n",
    "    \"turnover_total\", \"turnover_new_grad\", \"turnover_experienced\",\n",
    "    \"annual_income\",\n",
    "    \"nurse_count\", \"nurse_per_100k\",\n",
    "    \"hospital_count\", \"large_hospital_count\", \"large_hospital_ratio\", \"hospital_per_100k\",\n",
    "    \"rent_private\",\n",
    "    \"night_shift_72h_plus\",\n",
    "    \"population\", \"population_density\"\n",
    "]\n",
    "# å­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘\n",
    "cols_final = [c for c in cols_order if c in df_master.columns] + [c for c in df_master.columns if c not in cols_order]\n",
    "\n",
    "df_master = df_master[cols_final]\n",
    "df_master.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nâœ… SUCCESS! Dataset saved to:\\n{OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== è¨ºæ–­é–‹å§‹ ===\n",
      "\n",
      "1. é›¢è·ç‡ãƒ‡ãƒ¼ã‚¿ (Turnover) ã®ä¸­èº«ç¢ºèª:\n",
      "   [UTF-8èª­ã¿è¾¼ã¿] åˆ—åä¸€è¦§: ['prefecture', 'turnover_total', 'turnover_new_grad', 'turnover_experienced']\n",
      "   å…ˆé ­1è¡Œ:\n",
      "   prefecture  turnover_total  turnover_new_grad  turnover_experienced\n",
      "0        åŒ—æµ·é“            11.5                5.9                  16.6\n",
      "\n",
      "2. äººå£ãƒ‡ãƒ¼ã‚¿ (Density) ã®ä¸­èº«ç¢ºèª:\n",
      "   [Header=12] åˆ—åä¸€è¦§: ['èª¿æŸ»å¹´ ã‚³ãƒ¼ãƒ‰', 'èª¿æŸ»å¹´ è£œåŠ©ã‚³ãƒ¼ãƒ‰', 'èª¿æŸ»å¹´', 'åœ°åŸŸ ã‚³ãƒ¼ãƒ‰', 'åœ°åŸŸ è£œåŠ©ã‚³ãƒ¼ãƒ‰', 'åœ°åŸŸ', '/ï¼¡\\u3000äººå£ãƒ»ä¸–å¸¯', '#A011000_ç·äººå£ã€ä¸‡äººã€‘', '#A0110001_ç·äººå£ï¼ˆç”·ï¼‰ã€ä¸‡äººã€‘', '#A0110002_ç·äººå£ï¼ˆå¥³ï¼‰ã€ä¸‡äººã€‘', '#A01101_å…¨å›½ç·äººå£ã«å ã‚ã‚‹äººå£å‰²åˆï¼ˆA1101/A1101(å…¨å›½)ï¼‰ã€ï¼…ã€‘', '#A01201_ç·é¢ç©ï¼‘km2å½“ãŸã‚Šäººå£å¯†åº¦ã€äººã€‘', '#A01202_å¯ä½åœ°é¢ç©ï¼‘km2å½“ãŸã‚Šäººå£å¯†åº¦ã€äººã€‘', '#A0191002_å°†æ¥æ¨è¨ˆäººå£ï¼ˆ2025å¹´ï¼‰ã€äººã€‘', '#A0191003_å°†æ¥æ¨è¨ˆäººå£ï¼ˆ2030å¹´ï¼‰ã€äººã€‘', '#A0191004_å°†æ¥æ¨è¨ˆäººå£ï¼ˆ2035å¹´ï¼‰ã€äººã€‘', '#A0191005_å°†æ¥æ¨è¨ˆäººå£ï¼ˆ2040å¹´ï¼‰ã€äººã€‘', '#A0191006_å°†æ¥æ¨è¨ˆäººå£ï¼ˆ2045å¹´ï¼‰ã€äººã€‘', '#A0191007_å°†æ¥æ¨è¨ˆäººå£ï¼ˆ2050å¹´ï¼‰ã€äººã€‘', '#A02101_äººå£æ€§æ¯”ï¼ˆç·æ•°ï¼‰ï¼ˆA110101/A110102ï¼‰ã€â€ã€‘', '#A02102_äººå£æ€§æ¯”ï¼ˆ15æ­³æœªæº€äººå£ï¼‰(A130101/A130102)ã€â€ã€‘', '#A02103_äººå£æ€§æ¯”ï¼ˆ15ï½64æ­³äººå£ï¼‰(A130201/A130202)ã€â€ã€‘', '#A02104_äººå£æ€§æ¯”ï¼ˆ65æ­³ä»¥ä¸Šäººå£) (A130301/A130302)ã€â€ã€‘', '#A03501_15æ­³æœªæº€äººå£å‰²åˆã€ï¼…ã€‘', '#A03502_15ï½64æ­³äººå£å‰²åˆã€ï¼…ã€‘', '#A03503_65æ­³ä»¥ä¸Šäººå£å‰²åˆã€ï¼…ã€‘', '#A05101_äººå£å¢—æ¸›ç‡ï¼ˆï¼ˆA1101/A1101ï¼ˆ-1ï¼‰ï¼‰-1ï¼‰ã€ï¼…ã€‘', '#A05301_è»¢å…¥è¶…éç‡ï¼ˆæ—¥æœ¬äººç§»å‹•è€…ï¼‰ã€ï¼…ã€‘', '#A05302_è»¢å…¥ç‡ï¼ˆæ—¥æœ¬äººç§»å‹•è€…ï¼‰ã€ï¼…ã€‘', '#A05303_è»¢å‡ºç‡ï¼ˆæ—¥æœ¬äººç§»å‹•è€…ï¼‰ã€ï¼…ã€‘', '#A05307_è»¢å…¥è¶…éç‡ã€ï¼…ã€‘', '#A05308_è»¢å…¥ç‡ã€ï¼…ã€‘', '#A05309_è»¢å‡ºç‡ã€ï¼…ã€‘']\n",
      "   å…ˆé ­1è¡Œ:\n",
      "       èª¿æŸ»å¹´ ã‚³ãƒ¼ãƒ‰  èª¿æŸ»å¹´ è£œåŠ©ã‚³ãƒ¼ãƒ‰     èª¿æŸ»å¹´  åœ°åŸŸ ã‚³ãƒ¼ãƒ‰  åœ°åŸŸ è£œåŠ©ã‚³ãƒ¼ãƒ‰  åœ°åŸŸ  /ï¼¡ã€€äººå£ãƒ»ä¸–å¸¯  \\\n",
      "0  2023100000        NaN  2023å¹´åº¦       0       NaN  å…¨å›½       NaN   \n",
      "\n",
      "  #A011000_ç·äººå£ã€ä¸‡äººã€‘ #A0110001_ç·äººå£ï¼ˆç”·ï¼‰ã€ä¸‡äººã€‘ #A0110002_ç·äººå£ï¼ˆå¥³ï¼‰ã€ä¸‡äººã€‘  ...  \\\n",
      "0           12,435                6,049                6,386  ...   \n",
      "\n",
      "   #A03501_15æ­³æœªæº€äººå£å‰²åˆã€ï¼…ã€‘ #A03502_15ï½64æ­³äººå£å‰²åˆã€ï¼…ã€‘ #A03503_65æ­³ä»¥ä¸Šäººå£å‰²åˆã€ï¼…ã€‘  \\\n",
      "0                  11.4                  59.5                 29.1   \n",
      "\n",
      "  #A05101_äººå£å¢—æ¸›ç‡ï¼ˆï¼ˆA1101/A1101ï¼ˆ-1ï¼‰ï¼‰-1ï¼‰ã€ï¼…ã€‘ #A05301_è»¢å…¥è¶…éç‡ï¼ˆæ—¥æœ¬äººç§»å‹•è€…ï¼‰ã€ï¼…ã€‘  \\\n",
      "0                                 -0.48                        -   \n",
      "\n",
      "  #A05302_è»¢å…¥ç‡ï¼ˆæ—¥æœ¬äººç§»å‹•è€…ï¼‰ã€ï¼…ã€‘ #A05303_è»¢å‡ºç‡ï¼ˆæ—¥æœ¬äººç§»å‹•è€…ï¼‰ã€ï¼…ã€‘ #A05307_è»¢å…¥è¶…éç‡ã€ï¼…ã€‘  \\\n",
      "0                   1.79                   1.79                -   \n",
      "\n",
      "  #A05308_è»¢å…¥ç‡ã€ï¼…ã€‘  #A05309_è»¢å‡ºç‡ã€ï¼…ã€‘  \n",
      "0           2.05            2.05  \n",
      "\n",
      "[1 rows x 33 columns]\n",
      "\n",
      "=== è¨ºæ–­çµ‚äº† ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ãƒ‘ã‚¹è¨­å®šï¼ˆNotebookã®è¨­å®šã«åˆã‚ã›ã¾ã™ï¼‰\n",
    "BASE_DIR = Path(\"..\").resolve()\n",
    "RAW_DIR = BASE_DIR / \"data\" / \"raw\"\n",
    "\n",
    "print(\"=== è¨ºæ–­é–‹å§‹ ===\")\n",
    "\n",
    "# 1. é›¢è·ç‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã€Œæœ¬å½“ã®åˆ—åã€ã‚’ç¢ºèª\n",
    "# (ã“ã“ãŒã‚ºãƒ¬ã¦ã„ã‚‹ã¨ 'annual_income' ãŒä½œã‚‰ã‚Œãšã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™)\n",
    "print(\"\\n1. é›¢è·ç‡ãƒ‡ãƒ¼ã‚¿ (Turnover) ã®ä¸­èº«ç¢ºèª:\")\n",
    "try:\n",
    "    target_file = RAW_DIR / \"æ—¥æœ¬çœ‹è­·å”ä¼š_é›¢è·ç‡_éƒ½é“åºœçœŒåˆ¥_2023.csv\"\n",
    "    # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚‚ç–‘ã£ã¦æ¨™æº–ã¨Shift-JISä¸¡æ–¹è©¦ã™\n",
    "    try:\n",
    "        df_t = pd.read_csv(target_file)\n",
    "        print(\"   [UTF-8èª­ã¿è¾¼ã¿] åˆ—åä¸€è¦§:\", list(df_t.columns))\n",
    "    except:\n",
    "        df_t = pd.read_csv(target_file, encoding=\"cp932\")\n",
    "        print(\"   [Shift-JISèª­ã¿è¾¼ã¿] åˆ—åä¸€è¦§:\", list(df_t.columns))\n",
    "    print(\"   å…ˆé ­1è¡Œ:\\n\", df_t.head(1))\n",
    "except Exception as e:\n",
    "    print(\"   âŒ èª­ã¿è¾¼ã¿å¤±æ•—:\", e)\n",
    "\n",
    "# 2. äººå£ãƒ•ã‚¡ã‚¤ãƒ«ã®ã€Œèª­ã¿è¾¼ã¾ã‚Œæ–¹ã€ã‚’ç¢ºèª\n",
    "# (ã“ã“ãŒã‚ºãƒ¬ã¦ã„ã‚‹ã¨äººå£ãƒ‡ãƒ¼ã‚¿ãŒå…¨éƒ¨NaNã«ãªã‚Šã¾ã™)\n",
    "print(\"\\n2. äººå£ãƒ‡ãƒ¼ã‚¿ (Density) ã®ä¸­èº«ç¢ºèª:\")\n",
    "try:\n",
    "    target_file = RAW_DIR / \"ç·å‹™çœ_ç¤¾ä¼šç”Ÿæ´»çµ±è¨ˆæŒ‡æ¨™_äººå£å¯†åº¦_2023.csv\"\n",
    "    # header=12 ã§ã‚ã£ã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    df_d = pd.read_csv(target_file, encoding=\"utf-8-sig\", header=12)\n",
    "    print(\"   [Header=12] åˆ—åä¸€è¦§:\", list(df_d.columns))\n",
    "    print(\"   å…ˆé ­1è¡Œ:\\n\", df_d.head(1))\n",
    "except Exception as e:\n",
    "    print(\"   âŒ èª­ã¿è¾¼ã¿å¤±æ•—:\", e)\n",
    "\n",
    "print(\"\\n=== è¨ºæ–­çµ‚äº† ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
